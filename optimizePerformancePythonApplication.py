# hi·ªáu nƒÉng kh√¥ng ch·ªâ l√† t·ªëc ƒë·ªô m√† c√≤n l√† kh·∫£ nƒÉng m·ªü r·ªông, b·∫£o tr√¨ v√† s·ª≠ d·ª•ng t√†i nguy√™n hi·ªáu qu·∫£.
# Bottleneck l√† ƒëi·ªÉm y·∫øu trong h·ªá th·ªëng, n∆°i hi·ªáu su·∫•t b·ªã gi·ªõi h·∫°n ho·∫∑c gi·∫£m s√∫t.
# Bottleneck c√≥ th·ªÉ do nhi·ªÅu nguy√™n nh√¢n nh∆∞:
# - Thi·∫øu t√†i nguy√™n (CPU, RAM, I/O)
# - Thi·∫øt k·∫ø kh√¥ng t·ªëi ∆∞u (thu·∫≠t to√°n, c·∫•u tr√∫c d·ªØ li·ªáu)      
# - T·∫Øc ngh·∫Ωn trong lu·ªìng d·ªØ li·ªáu (ƒë·ªçc/ghi ƒëƒ©a, truy v·∫•n c∆° s·ªü d·ªØ li·ªáu, ƒë·ªçc ghi file l·ªõn)


# t·ªëi ∆∞u hi·ªáu nƒÉng ·ª©ng d·ª•ng Python : 

# 1. S·ª≠ d·ª•ng chunk ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu l·ªõn, tr√°nh t·∫£i to√†n b·ªô d·ªØ li·ªáu v√†o RAM c√πng m·ªôt l√∫c.
import pandas as pd
# from sqlalchemy import create_engine
# import psycopg2
# from psycopg2 import sql
# import time
# from contextlib import closing

def process_large_data_in_chunks(file_path, chunk_size=10000):
    """
    X·ª≠ l√Ω d·ªØ li·ªáu l·ªõn theo t·ª´ng kh·ªëi ƒë·ªÉ tr√°nh qu√° t·∫£i b·ªô nh·ªõ.
    """
    for chunk in pd.read_csv(file_path, chunksize=chunk_size):
        # Th·ª±c hi·ªán c√°c ph√©p to√°n tr√™n t·ª´ng kh·ªëi d·ªØ li·ªáu
        process_chunk(chunk)

def process_chunk(chunk):
    """
    X·ª≠ l√Ω t·ª´ng kh·ªëi d·ªØ li·ªáu.
    """
    # V√≠ d·ª•: L·ªçc d·ªØ li·ªáu, t√≠nh to√°n, v.v.
    filtered_chunk = chunk[chunk['status'] == 'CLOSED']
    # Ti·∫øp t·ª•c x·ª≠ l√Ω d·ªØ li·ªáu ƒë√£ l·ªçc
    print(filtered_chunk.head())


# 2. Batch insert d√πng executemany ƒë·ªÉ ghi nhi·ªÅu b·∫£n ghi v√†o 1 l·∫ßn.
def insert_data_batch(conn, data):
    """
    Ch√®n d·ªØ li·ªáu v√†o c∆° s·ªü d·ªØ li·ªáu theo l√¥.
    """
    with closing(conn.cursor()) as cursor:
        insert_query = sql.SQL("INSERT INTO orders (order_id, order_date, customer_id, status) VALUES %s")
        psycopg2.extras.execute_values(cursor, insert_query, data)
        conn.commit()

# 3. S·ª≠ d·ª•ng multiprocessing ƒë·ªÉ x·ª≠ l√Ω song song.
import multiprocessing
def process_data_in_parallel(data):
    """
    X·ª≠ l√Ω d·ªØ li·ªáu song song b·∫±ng multiprocessing.
    """
    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:
        results = pool.map(process_chunk, data)
    return results

# Challenge 
import pyodbc
import time
import pandas as pd
from memory_profiler import profile
import psutil
import os

connection_string = (
    "DRIVER={SQL Server};"
    "SERVER=DESKTOP-O1721LD;"
    "DATABASE=MIKI_SHOP;"
    "UID=sa;"
    "PWD=d11052003;"
)

process = psutil.Process(os.getpid())

@profile
def main():
    start_time = time.time()
    print("Loading CSV...")

    df = pd.read_csv('./data/retail_db/orders/part-00000', names=['order_id', 'order_date', 'customer_id', 'status'])
    print(f"CSV Loaded: {len(df)} rows")
    print(f"RAM after loading CSV: {process.memory_info().rss / 1024 ** 2:.2f} MB")

    with pyodbc.connect(connection_string) as conn:
        cursor = conn.cursor()

        insert_start = time.time()
        for index, row in df.iterrows():
            if index % 1000 == 0 and index > 0:
                print(f"Inserted {index} rows. RAM: {process.memory_info().rss / 1024 ** 2:.2f} MB, CPU: {process.cpu_percent()}%")
            cursor.execute(
                "INSERT INTO orders_csv (order_id, order_date, customer_id, status) VALUES (?, ?, ?, ?)",
                row["order_id"], row["order_date"], row["customer_id"], row["status"]
            )
        conn.commit()
        insert_end = time.time()

    end_time = time.time()
    print(f"\nTotal Time: {end_time - start_time:.2f} seconds")
    print(f"Insert Time: {insert_end - insert_start:.2f} seconds")
    print(f"Final RAM usage: {process.memory_info().rss / 1024 ** 2:.2f} MB")
    print(f"Peak CPU usage: {process.cpu_percent()}%")

#if __name__ == "__main__":
    #main()
    #print("Main function")


import pandas as pd
import time
import psutil
import os
from multiprocessing import Pool, cpu_count
from db_utils import insert_batch  

CHUNK_SIZE = 10000

def process_csv_in_chunks(file_path):
    start_time = time.time()
    process = psutil.Process(os.getpid())
    total_inserted = 0

    data_chunks = []
    for chunk in pd.read_csv(file_path, names=['order_id', 'order_date', 'customer_id', 'status'], chunksize=CHUNK_SIZE):
        data_chunks.append(chunk.values.tolist())

    print(f"üîÑ T·ªïng s·ªë chunk: {len(data_chunks)} ‚Äî b·∫Øt ƒë·∫ßu ghi...")

    with Pool(cpu_count()) as pool:
        results = pool.map(insert_batch, data_chunks)

    total_inserted = sum(results)
    end_time = time.time()

    print(f"\n‚úÖ T·ªïng b·∫£n ghi insert: {total_inserted}")
    print(f"‚è±Ô∏è Th·ªùi gian: {end_time - start_time:.2f}s")
    print(f"üß† RAM: {process.memory_info().rss / 1024**2:.2f} MB")

if __name__ == "__main__":
    process_csv_in_chunks('./data/retail_db/orders/part-00000')

